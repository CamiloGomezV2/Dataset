{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto II: Generación automatica de texto\n",
    "\n",
    "En este poryecto el objetivo es desarrollar un modelo de generación aunotática de texto utilizando redes recurrentes.Para este modelo deben realizar los siguientes pasos:\n",
    "\n",
    "1. Importar un corpus (texto con el que van a entrenar el modelo). Este corpus debe ser grande y contener mas de 100 mil palabras. Para este proyecto se pedira que entrenen con dos corpus de estilo diferente. Por ejemplo, un corpus con las obras de gabriel garcia marquez, otro corpus con poemas.\n",
    "2. Preprocesar el texto. Tenga en cuenta que la entrada debe ser lo más homogenea posible, para poder entrenar las redes. Algunos modelos eliminan la puntuación, sin embargo, si esto se hace el texto generado no tendra puntuación. Yo recomiendo dejar estos elementos. Al igual que los saltos de lineas si entrenan poemas, o incluso en el texto para generar el punto aparte.\n",
    "3. Preparar los datos de entrenamiento, esto inclutye los wordembeddings, o la generación de indices que puedan ser utilziados en un acapa de embedding.\n",
    "4. Diseño de la arquitectura de red. Yo aconsejo utilizar un acapa de embeddings, para que el algoritmo por si solo aprenda el embedding apropiado para la tarea.\n",
    "5. generación de texto. Tenga en cuenta que al entrenar se entrena en modo profesor, pero al generar se debe retroalimentar la salida del modelo como entrada para producir las palabras siguientes. Para parar su ejecución pueden generar un token de finalizacion /<end/> y solo parar cuando este token aparezca, si este es el caso ese token debe estar en el diccionario y en el texto que esten procesando. Otra forma es generar x número de palabras y parar la ejecución una vez se haya cumplido con ese número de palabras.\n",
    "6. Para ejecutar el programa deben proporcionar una semilla, esta semilla debe contener palabras que esten en el vocabulario con el que se entreno, una vez hecho esto se puede correr el modelo.\n",
    "7. En la entrega deben especificar paso a paso el proceso realizado, comparar los resultados del texto generado con ambos modelos entrenados (uno para cada corpus) y discutir a profundidad los resultados obtenidos, problemas y posibles mejoras.\n",
    "\n",
    "Dado que este proyecto requiere el uso de temas no vistos a profundidad en clase, les proporciono el siguiente blog que pueden usar como base para el desarrollo del proyecto. Les recomiendo lo lean, lo sigan paso a paso, ejecuten el código que allí se encuentra y luego hagan las modificaciones necesarias para el desarrollo de su proyecto. El blog lo pueden encontrar en este [link](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/). \n",
    "\n",
    "El proyecto lo deben entregar el Lunes 24 de abril\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como primer paso, se definen cuatro funciones. La primera función carga y lee los archivos de texto y retorna una única lista. La segunda función, se encarga de limpiar el texto, eliminando algunos caracteres especiales y castear (casting) las palabras a minúscula, esta función retorna una lista con todas las palabras de acuerdo con el orden que fueron escritas. La tercera función se encarga de guardar el texto ya procesado en un nuevo archivo de texto. Por último, la cuarta función recibe las palabras (tokens) y una longitud definida, dicha longitud define el tamaño de la secuencia que se generara a partir de las palabras de entrada. Estas cuatro funciones componen todo el preprocesamiento de texto para cada corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(filenames):\n",
    "    complete_text = []\n",
    "    for filename in filenames:\n",
    "        file = open(filename, 'r')\n",
    "        text = file.read()\n",
    "        complete_text += text\n",
    "        file.close()\n",
    "    return text\n",
    "\n",
    "def clean_text(doc):\n",
    "    doc = doc.replace('--', '')\n",
    "    doc = doc.replace('—', '')\n",
    "    doc = doc.replace('-', '')\n",
    "    doc = doc.replace('«', '')\n",
    "    doc = doc.replace('»', '')\n",
    "    doc = doc.replace('.', '. ')\n",
    "    doc = doc.replace(' .', '.')\n",
    "    doc = doc.replace('[', '')\n",
    "    doc = doc.replace(']', '')\n",
    "    doc = doc.replace('...', '')\n",
    "    doc = doc.replace(' !', '!')\n",
    "    doc = doc.replace(' ?', '?')\n",
    "    doc = doc.replace(' ,', ',')\n",
    "    doc = doc.replace('( ', '(')\n",
    "    doc = doc.replace(' )', ')')\n",
    "    tokens = doc.split()\n",
    "    tokens = [token.lower() for token in tokens if token != '']\n",
    "    print(\"Clean text completed\")\n",
    "    print(f'Total tokens: {len(tokens)}')\n",
    "    print(f'Unique tokens: {len(set(tokens))}')\n",
    "    return tokens\n",
    "\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "def to_sequences(length, tokens):\n",
    "    sequences = []\n",
    "    for i in range(length, len(tokens)):\n",
    "        seq = tokens[i-length:i]\n",
    "        if len(seq) == length:\n",
    "            line = ' '.join(seq)\n",
    "            sequences.append(line)\n",
    "        else:\n",
    "            sequences\n",
    "    print(\"Sequences formed\")\n",
    "    print(f'Total Sequences: {len(sequences)}')\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus 1 - Primer Modelo\n",
    "Como primer corpus, se consideró la saga completa de Harry Potter. Se comienza cargando los siete libros de Harry Potter y se realiza el preprocesamiento. Observe que se cuenta con 208.782 palabras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean text completed\n",
      "Total tokens: 208782\n",
      "Unique tokens: 26661\n",
      "Sequences formed\n",
      "Total Sequences: 208731\n"
     ]
    }
   ],
   "source": [
    "in_filenames = ['Books_data/HarryPotter/J.K. Rowling - Harry Potter 1 - La Piedra Filosofal.txt',\n",
    "               'Books_data/HarryPotter/J.K. Rowling - Harry Potter 2 - La Cámara Secreta.txt',\n",
    "               'Books_data/HarryPotter/J.K. Rowling - Harry Potter 3 - El Prisionero de Azkaban.txt',\n",
    "               'Books_data/HarryPotter/J.K. Rowling - Harry Potter 4 - El Cáliz de Fuego.txt',\n",
    "               'Books_data/HarryPotter/J.K. Rowling - Harry Potter 5 - La Orden del Fenix.txt',\n",
    "               'Books_data/HarryPotter/J.K. Rowling - Harry Potter 6 - El Misterio del Príncipe.txt',\n",
    "               'Books_data/HarryPotter/J.K. Rowling - Harry Potter 7 - Las Reliquias de la Muerte.txt']\n",
    "doc = load_docs(in_filenames)\n",
    "tokens = clean_text(doc)\n",
    "lines = to_sequences(51, tokens)\n",
    "\n",
    "out_filename = 'HarryPoter_CleanText.txt'\n",
    "save_doc(lines, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, se configura el uso de GPU para acelerar el tiempo de ejecución (entrenamiento) del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el archivo de texto que contiene el texto ya procesado y se realiza la tokenizacion del mismo. También, se definen las secuencias de acuerdo con los tokens definidos (índices). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "\n",
    "in_filename = 'HarryPoter_CleanText.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "sequences = [sequence for sequence in sequences if len(sequence) == 51]\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define el modelo el cual consta de una capa de embedding, dos capas de Long short-term memory (LSTM) y 2 capas densas, la primera con una función de activación relu y la segunda con una función de activación softmax. Como optimizador se utiliza Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            886050    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 80)            41920     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 80)                51520     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 17721)             903771    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,887,311\n",
      "Trainable params: 1,887,311\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "seq_length = 50 #X.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(80, return_sequences=True))\n",
    "model.add(LSTM(80))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado el tamaño de los datos, la arquitectura del modelo y la capacidad computacional disponible, los datos son divididos en 4 secciones y cada sección se presenta al modelo con un batch size de 128 y 40 épocas. Los resultados del entrenamiento son almacenados en los archivos `model_HarryPotter.h5` y `tokenizer_HarryPotter.pkl`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 05:51:43.226467: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n",
      "2022-04-25 05:51:47.641404: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "408/408 [==============================] - 10s 15ms/step - loss: 7.3237 - accuracy: 0.0460\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.7363 - accuracy: 0.0481\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.4804 - accuracy: 0.0622\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.3168 - accuracy: 0.0681\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.1756 - accuracy: 0.0766\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.0242 - accuracy: 0.0905\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.8908 - accuracy: 0.1039\n",
      "Epoch 8/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.7773 - accuracy: 0.1114\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.6762 - accuracy: 0.1192\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.5797 - accuracy: 0.1247\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.4869 - accuracy: 0.1313\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.3943 - accuracy: 0.1352\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.3030 - accuracy: 0.1393\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.2161 - accuracy: 0.1421\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.1303 - accuracy: 0.1466\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.0523 - accuracy: 0.1500\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.9748 - accuracy: 0.1544\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.9013 - accuracy: 0.1575\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.8265 - accuracy: 0.1601\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.7565 - accuracy: 0.1653\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.6906 - accuracy: 0.1669\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.6230 - accuracy: 0.1732\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.5614 - accuracy: 0.1775\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.5010 - accuracy: 0.1800\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.4452 - accuracy: 0.1810\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.3891 - accuracy: 0.1861\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.3343 - accuracy: 0.1916\n",
      "Epoch 28/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.2841 - accuracy: 0.1931\n",
      "Epoch 29/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.2694 - accuracy: 0.1954\n",
      "Epoch 30/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.1989 - accuracy: 0.2000\n",
      "Epoch 31/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.1531 - accuracy: 0.2050\n",
      "Epoch 32/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.1088 - accuracy: 0.2082\n",
      "Epoch 33/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.0626 - accuracy: 0.2126\n",
      "Epoch 34/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.0224 - accuracy: 0.2158\n",
      "Epoch 35/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.9837 - accuracy: 0.2191\n",
      "Epoch 36/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.9437 - accuracy: 0.2241\n",
      "Epoch 37/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.9028 - accuracy: 0.2268\n",
      "Epoch 38/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.8651 - accuracy: 0.2317\n",
      "Epoch 39/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.8301 - accuracy: 0.2347\n",
      "Epoch 40/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.7983 - accuracy: 0.2377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 05:56:00.605752: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n",
      "2022-04-25 05:56:04.979942: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 7.1399 - accuracy: 0.0683\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.2375 - accuracy: 0.0851\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.0628 - accuracy: 0.0946\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.9292 - accuracy: 0.1058\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.8062 - accuracy: 0.1126\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.6930 - accuracy: 0.1201\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.5824 - accuracy: 0.1253\n",
      "Epoch 8/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.4786 - accuracy: 0.1306\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.3759 - accuracy: 0.1369\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.2786 - accuracy: 0.1436\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.1800 - accuracy: 0.1507\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.0886 - accuracy: 0.1582\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.9977 - accuracy: 0.1646\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.9112 - accuracy: 0.1715\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.8288 - accuracy: 0.1779\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.7471 - accuracy: 0.1830\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.6692 - accuracy: 0.1885\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.5940 - accuracy: 0.1953\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.5170 - accuracy: 0.1996\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.4451 - accuracy: 0.2057\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.3723 - accuracy: 0.2097\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.3039 - accuracy: 0.2152\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.2367 - accuracy: 0.2200\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.1718 - accuracy: 0.2247\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.1088 - accuracy: 0.2299\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.0448 - accuracy: 0.2360\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.9870 - accuracy: 0.2395\n",
      "Epoch 28/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.9284 - accuracy: 0.2459\n",
      "Epoch 29/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.8712 - accuracy: 0.2493\n",
      "Epoch 30/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.8148 - accuracy: 0.2553\n",
      "Epoch 31/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.7642 - accuracy: 0.2591\n",
      "Epoch 32/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.7089 - accuracy: 0.2643\n",
      "Epoch 33/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.6580 - accuracy: 0.2700\n",
      "Epoch 34/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.6111 - accuracy: 0.2748\n",
      "Epoch 35/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.5651 - accuracy: 0.2803\n",
      "Epoch 36/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.5210 - accuracy: 0.2851\n",
      "Epoch 37/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.4767 - accuracy: 0.2904\n",
      "Epoch 38/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.4319 - accuracy: 0.2961\n",
      "Epoch 39/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.3961 - accuracy: 0.3020\n",
      "Epoch 40/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.3525 - accuracy: 0.3069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 06:00:14.661106: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 7.4634 - accuracy: 0.0597\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.5568 - accuracy: 0.0858\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.2718 - accuracy: 0.0980\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.0812 - accuracy: 0.1047\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.9322 - accuracy: 0.1135\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.8036 - accuracy: 0.1201\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.6877 - accuracy: 0.1253\n",
      "Epoch 8/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.5819 - accuracy: 0.1297\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.4822 - accuracy: 0.1354\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.3911 - accuracy: 0.1412\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.3017 - accuracy: 0.1462\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.2156 - accuracy: 0.1519\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.1364 - accuracy: 0.1561\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.0567 - accuracy: 0.1631\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.9817 - accuracy: 0.1674\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.9093 - accuracy: 0.1730\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.8406 - accuracy: 0.1800\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.7955 - accuracy: 0.1835\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.7085 - accuracy: 0.1910\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.6419 - accuracy: 0.1970\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.5813 - accuracy: 0.2013\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.5211 - accuracy: 0.2073\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.4681 - accuracy: 0.2123\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.4134 - accuracy: 0.2175\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.3567 - accuracy: 0.2222\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.3066 - accuracy: 0.2269\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.2595 - accuracy: 0.2309\n",
      "Epoch 28/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.2112 - accuracy: 0.2353\n",
      "Epoch 29/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.1616 - accuracy: 0.2398\n",
      "Epoch 30/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.1131 - accuracy: 0.2438\n",
      "Epoch 31/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.0719 - accuracy: 0.2495\n",
      "Epoch 32/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.0249 - accuracy: 0.2523\n",
      "Epoch 33/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.9846 - accuracy: 0.2561\n",
      "Epoch 34/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.9401 - accuracy: 0.2619\n",
      "Epoch 35/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.8977 - accuracy: 0.2664\n",
      "Epoch 36/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.8603 - accuracy: 0.2686\n",
      "Epoch 37/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.8212 - accuracy: 0.2748\n",
      "Epoch 38/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.7803 - accuracy: 0.2785\n",
      "Epoch 39/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.7397 - accuracy: 0.2825\n",
      "Epoch 40/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.7031 - accuracy: 0.2867\n",
      "Epoch 1/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 7.5127 - accuracy: 0.0699\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.8410 - accuracy: 0.0894\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.5256 - accuracy: 0.0997\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.3255 - accuracy: 0.1059\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.1736 - accuracy: 0.1106\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 6.0500 - accuracy: 0.1154\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.9453 - accuracy: 0.1191\n",
      "Epoch 8/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.8438 - accuracy: 0.1244\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.7540 - accuracy: 0.1286\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.6672 - accuracy: 0.1331\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.5826 - accuracy: 0.1388\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.5096 - accuracy: 0.1418\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.4296 - accuracy: 0.1495\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.3561 - accuracy: 0.1541\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.2871 - accuracy: 0.1590\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.2173 - accuracy: 0.1650\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.1527 - accuracy: 0.1698\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.0878 - accuracy: 0.1750\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 5.0274 - accuracy: 0.1799\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.9648 - accuracy: 0.1846\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.9095 - accuracy: 0.1912\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.8528 - accuracy: 0.1943\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.7972 - accuracy: 0.1995\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.7455 - accuracy: 0.2018\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.6932 - accuracy: 0.2075\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.6433 - accuracy: 0.2126\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.5958 - accuracy: 0.2153\n",
      "Epoch 28/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.5477 - accuracy: 0.2213\n",
      "Epoch 29/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.4981 - accuracy: 0.2248\n",
      "Epoch 30/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.4538 - accuracy: 0.2285\n",
      "Epoch 31/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.4129 - accuracy: 0.2333\n",
      "Epoch 32/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.3682 - accuracy: 0.2376\n",
      "Epoch 33/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.3227 - accuracy: 0.2417\n",
      "Epoch 34/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.2836 - accuracy: 0.2454\n",
      "Epoch 35/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.2405 - accuracy: 0.2497\n",
      "Epoch 36/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.2919 - accuracy: 0.2438\n",
      "Epoch 37/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.1679 - accuracy: 0.2553\n",
      "Epoch 38/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.1222 - accuracy: 0.2600\n",
      "Epoch 39/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.0866 - accuracy: 0.2633\n",
      "Epoch 40/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 4.0500 - accuracy: 0.2678\n"
     ]
    }
   ],
   "source": [
    "size_data = len(sequences)//4\n",
    "\n",
    "for i in range(size_data,len(sequences)+1, size_data):\n",
    "    sub_sequences = sequences[i-size_data:i]\n",
    "    sequences_aux = array(sub_sequences)\n",
    "    \n",
    "    X, y = sequences_aux[:,:-1], sequences_aux[:,-1]\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "    model.fit(X, y, batch_size=128, epochs=40)\n",
    "    \n",
    "# save the model to file\n",
    "model.save('model_HarryPotter.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer_HarryPotter.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus 1 - Segundo Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            886050    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 20)            5680      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 20)                3280      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                630       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 17721)             549351    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,444,991\n",
      "Trainable params: 1,444,991\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "seq_length = 50 #X.shape[1]\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model3.add(LSTM(20, return_sequences=True))\n",
    "model3.add(LSTM(20))\n",
    "model3.add(Dense(30, activation='relu'))\n",
    "model3.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model3.summary())\n",
    "# compile model\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 14:27:45.196313: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n",
      "2022-04-25 14:27:49.631517: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "408/408 [==============================] - 10s 14ms/step - loss: 7.4237 - accuracy: 0.0432\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.7387 - accuracy: 0.0463\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 6.6442 - accuracy: 0.0490\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 6.5204 - accuracy: 0.0612\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 6.3776 - accuracy: 0.0662\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 6.2511 - accuracy: 0.0693\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 6.1342 - accuracy: 0.0731\n",
      "Epoch 8/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 6.0489 - accuracy: 0.0742\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.9795 - accuracy: 0.0792\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.9151 - accuracy: 0.0818\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.8484 - accuracy: 0.0862\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.7744 - accuracy: 0.0915\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.6931 - accuracy: 0.0980\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.6056 - accuracy: 0.1050\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.5212 - accuracy: 0.1108\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.4390 - accuracy: 0.1181\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.3595 - accuracy: 0.1246\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.2808 - accuracy: 0.1291\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.2011 - accuracy: 0.1359\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.1201 - accuracy: 0.1391\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.0414 - accuracy: 0.1454\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 4.9622 - accuracy: 0.1504\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.8814 - accuracy: 0.1549\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.8026 - accuracy: 0.1595\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.7244 - accuracy: 0.1645\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.6453 - accuracy: 0.1694\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.5680 - accuracy: 0.1743\n",
      "Epoch 28/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.4870 - accuracy: 0.1788\n",
      "Epoch 29/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.4130 - accuracy: 0.1834\n",
      "Epoch 30/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.3363 - accuracy: 0.1873\n",
      "Epoch 31/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.2659 - accuracy: 0.1933\n",
      "Epoch 32/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1983 - accuracy: 0.1972\n",
      "Epoch 33/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1351 - accuracy: 0.2023\n",
      "Epoch 34/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.0707 - accuracy: 0.2058\n",
      "Epoch 35/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.0118 - accuracy: 0.2127\n",
      "Epoch 36/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.9530 - accuracy: 0.2185\n",
      "Epoch 37/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.9003 - accuracy: 0.2237\n",
      "Epoch 38/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.8458 - accuracy: 0.2315\n",
      "Epoch 39/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.7960 - accuracy: 0.2367\n",
      "Epoch 40/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.7494 - accuracy: 0.2417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 14:31:35.767323: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n",
      "2022-04-25 14:31:40.157860: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 7.2226 - accuracy: 0.0613\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.2547 - accuracy: 0.0826\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.0671 - accuracy: 0.0921\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.9107 - accuracy: 0.1024\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.7693 - accuracy: 0.1113\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.6398 - accuracy: 0.1208\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.5148 - accuracy: 0.1290\n",
      "Epoch 8/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.3956 - accuracy: 0.1376\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.2824 - accuracy: 0.1457\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.1720 - accuracy: 0.1541\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.0654 - accuracy: 0.1610\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.9622 - accuracy: 0.1689\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.8647 - accuracy: 0.1773\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.7701 - accuracy: 0.1851\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.6758 - accuracy: 0.1914\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.5889 - accuracy: 0.1996\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.5021 - accuracy: 0.2052\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.4191 - accuracy: 0.2104\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.3369 - accuracy: 0.2180\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.2617 - accuracy: 0.2238\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1863 - accuracy: 0.2300\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1115 - accuracy: 0.2363\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.0377 - accuracy: 0.2420\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.9644 - accuracy: 0.2481\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.8967 - accuracy: 0.2555\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.8307 - accuracy: 0.2604\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.7624 - accuracy: 0.2651\n",
      "Epoch 28/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.6982 - accuracy: 0.2731\n",
      "Epoch 29/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.6354 - accuracy: 0.2800\n",
      "Epoch 30/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.5744 - accuracy: 0.2866\n",
      "Epoch 31/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.5141 - accuracy: 0.2941\n",
      "Epoch 32/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.4567 - accuracy: 0.2987\n",
      "Epoch 33/40\n",
      "408/408 [==============================] - 6s 13ms/step - loss: 3.3999 - accuracy: 0.3057\n",
      "Epoch 34/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.3461 - accuracy: 0.3125\n",
      "Epoch 35/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.2914 - accuracy: 0.3193\n",
      "Epoch 36/40\n",
      "408/408 [==============================] - 6s 13ms/step - loss: 3.2361 - accuracy: 0.3270\n",
      "Epoch 37/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.1841 - accuracy: 0.3323\n",
      "Epoch 38/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.1336 - accuracy: 0.3415\n",
      "Epoch 39/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.0892 - accuracy: 0.3451\n",
      "Epoch 40/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.0416 - accuracy: 0.3554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 14:35:20.829574: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3697096788 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 7.7507 - accuracy: 0.0515\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.6059 - accuracy: 0.0806\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.2691 - accuracy: 0.0919\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 6.0496 - accuracy: 0.1003\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.8799 - accuracy: 0.1076\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 5.7289 - accuracy: 0.1147\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.5914 - accuracy: 0.1218\n",
      "Epoch 8/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.4638 - accuracy: 0.1287\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.3418 - accuracy: 0.1363\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.2269 - accuracy: 0.1454\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 6s 13ms/step - loss: 5.1175 - accuracy: 0.1541\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 6s 13ms/step - loss: 5.0147 - accuracy: 0.1632\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.9125 - accuracy: 0.1706\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.8177 - accuracy: 0.1796\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.7277 - accuracy: 0.1882\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.6398 - accuracy: 0.1955\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.5548 - accuracy: 0.2022\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.4724 - accuracy: 0.2106\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.3959 - accuracy: 0.2171\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.3224 - accuracy: 0.2245\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 6s 13ms/step - loss: 4.2465 - accuracy: 0.2317\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1755 - accuracy: 0.2391\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1083 - accuracy: 0.2466\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.0402 - accuracy: 0.2535\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.9745 - accuracy: 0.2612\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.9109 - accuracy: 0.2676\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.8519 - accuracy: 0.2741\n",
      "Epoch 28/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.7938 - accuracy: 0.2808\n",
      "Epoch 29/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.7355 - accuracy: 0.2861\n",
      "Epoch 30/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.6790 - accuracy: 0.2955\n",
      "Epoch 31/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.6258 - accuracy: 0.3010\n",
      "Epoch 32/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.5729 - accuracy: 0.3071\n",
      "Epoch 33/40\n",
      "408/408 [==============================] - 6s 13ms/step - loss: 3.5184 - accuracy: 0.3136\n",
      "Epoch 34/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.4723 - accuracy: 0.3204\n",
      "Epoch 35/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.4243 - accuracy: 0.3263\n",
      "Epoch 36/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.3731 - accuracy: 0.3314\n",
      "Epoch 37/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.3246 - accuracy: 0.3379\n",
      "Epoch 38/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.2748 - accuracy: 0.3444\n",
      "Epoch 39/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.2327 - accuracy: 0.3507\n",
      "Epoch 40/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.1910 - accuracy: 0.3552\n",
      "Epoch 1/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 7.7490 - accuracy: 0.0565\n",
      "Epoch 2/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.7636 - accuracy: 0.0850\n",
      "Epoch 3/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.4740 - accuracy: 0.0977\n",
      "Epoch 4/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.2704 - accuracy: 0.1059\n",
      "Epoch 5/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 6.1039 - accuracy: 0.1138\n",
      "Epoch 6/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.9553 - accuracy: 0.1207\n",
      "Epoch 7/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.8144 - accuracy: 0.1289\n",
      "Epoch 8/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.6865 - accuracy: 0.1352\n",
      "Epoch 9/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.5624 - accuracy: 0.1424\n",
      "Epoch 10/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.4440 - accuracy: 0.1504\n",
      "Epoch 11/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.3307 - accuracy: 0.1586\n",
      "Epoch 12/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.2234 - accuracy: 0.1662\n",
      "Epoch 13/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.1236 - accuracy: 0.1749\n",
      "Epoch 14/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 5.0253 - accuracy: 0.1818\n",
      "Epoch 15/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.9332 - accuracy: 0.1900\n",
      "Epoch 16/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.8428 - accuracy: 0.1959\n",
      "Epoch 17/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.7573 - accuracy: 0.2038\n",
      "Epoch 18/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.6757 - accuracy: 0.2109\n",
      "Epoch 19/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.5945 - accuracy: 0.2169\n",
      "Epoch 20/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.5222 - accuracy: 0.2235\n",
      "Epoch 21/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.4482 - accuracy: 0.2302\n",
      "Epoch 22/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.3772 - accuracy: 0.2367\n",
      "Epoch 23/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.3103 - accuracy: 0.2439\n",
      "Epoch 24/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.2446 - accuracy: 0.2504\n",
      "Epoch 25/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1816 - accuracy: 0.2556\n",
      "Epoch 26/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.1190 - accuracy: 0.2635\n",
      "Epoch 27/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.0615 - accuracy: 0.2668\n",
      "Epoch 28/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 4.0016 - accuracy: 0.2741\n",
      "Epoch 29/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.9448 - accuracy: 0.2795\n",
      "Epoch 30/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.8911 - accuracy: 0.2841\n",
      "Epoch 31/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.8389 - accuracy: 0.2885\n",
      "Epoch 32/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.7871 - accuracy: 0.2961\n",
      "Epoch 33/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.7358 - accuracy: 0.3004\n",
      "Epoch 34/40\n",
      "408/408 [==============================] - 6s 15ms/step - loss: 3.6853 - accuracy: 0.3061\n",
      "Epoch 35/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.6431 - accuracy: 0.3111\n",
      "Epoch 36/40\n",
      "408/408 [==============================] - 6s 14ms/step - loss: 3.5960 - accuracy: 0.3163\n",
      "Epoch 37/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.5506 - accuracy: 0.3194\n",
      "Epoch 38/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.5038 - accuracy: 0.3247\n",
      "Epoch 39/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.4621 - accuracy: 0.3301\n",
      "Epoch 40/40\n",
      "408/408 [==============================] - 5s 13ms/step - loss: 3.4221 - accuracy: 0.3343\n"
     ]
    }
   ],
   "source": [
    "size_data = len(sequences)//4\n",
    "\n",
    "for i in range(size_data,len(sequences)+1, size_data):\n",
    "    sub_sequences = sequences[i-size_data:i]\n",
    "    sequences_aux = array(sub_sequences)\n",
    "    \n",
    "    X, y = sequences_aux[:,:-1], sequences_aux[:,-1]\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "    model3.fit(X, y, batch_size=128, epochs=40)\n",
    "    \n",
    "# save the model to file\n",
    "model3.save('model2_HarryPotter.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer2_HarryPotter.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Corpus 2 - Primer Modelo\n",
    "\n",
    "Para el segundo modelo, se consideró un corpus con el libro El Código Da Vinci. De igual forma al caso anterior, comenzamos cargando el texto y realizando el debido preprocesamiento para así limpiar los datos. Observe que, para este caso, se tienen 153.890 palabras, lo cual es una cuarta parte menos al caso anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean text completed\n",
      "Total tokens: 153890\n",
      "Unique tokens: 22182\n",
      "Sequences formed\n",
      "Total Sequences: 153839\n"
     ]
    }
   ],
   "source": [
    "in_filename = 'Books_data/El Codigo Da Vinci.txt'\n",
    "doc = load_doc(in_filename)\n",
    "tokens = clean_text(doc)\n",
    "lines = to_sequences(51, tokens)\n",
    "\n",
    "out_filename = 'CodigoDaVinci_CleanText.txt'\n",
    "save_doc(lines, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de correr el modelo, configuramos el uso de la GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 21:40:57.177338: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-26 21:40:57.177374: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-04-26 21:41:01.315944: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-26 21:41:01.315987: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-26 21:41:01.316017: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (default): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_76/3589921078.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mphysical_devices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphysical_devices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el archivo que contiene los datos limpios, se realiza la tokenizacion y se crean las secuencias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "\n",
    "\n",
    "in_filename = 'CodigoDaVinci_CleanText.txt'\n",
    "doc2 = load_doc(in_filename)\n",
    "lines2 = doc2.split('\\n')\n",
    "\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(lines2)\n",
    "sequences2 = tokenizer2.texts_to_sequences(lines2)\n",
    "sequences2 = [sequence for sequence in sequences2 if len(sequence) == 51]\n",
    "vocab_size2 = len(tokenizer2.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define el modelo. Observe que el modelo es igual al anterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            774550    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 80)            41920     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 80)                51520     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                4050      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 15491)             790041    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,662,081\n",
      "Trainable params: 1,662,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "seq_length2 = 50 #X.shape[1]\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size2, 50, input_length=seq_length2))\n",
    "model2.add(LSTM(80, return_sequences=True))\n",
    "model2.add(LSTM(80))\n",
    "model2.add(Dense(50, activation='relu'))\n",
    "model2.add(Dense(vocab_size2, activation='softmax'))\n",
    "print(model2.summary())\n",
    "# compile model\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el caso de este corpus, dado que es más pequeño que el primero, los datos se dividieron en 3 secciones y, de igual manera, se entró con un batch size de 128 con la diferencia de que el entrenamiento se realiza por 50 épocas. Los resultados del modelo se almacenan en los archivos `model_DaVinci.h5` y `tokenizer_DaVinci.pkl` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 06:22:06.160705: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n",
      "2022-04-25 06:22:09.421024: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "342/342 [==============================] - 9s 14ms/step - loss: 7.2735 - accuracy: 0.0508\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.6792 - accuracy: 0.0541\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.4477 - accuracy: 0.0726\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.2327 - accuracy: 0.0801\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.1081 - accuracy: 0.0842\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.9814 - accuracy: 0.0895\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.8410 - accuracy: 0.1011\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.7174 - accuracy: 0.1119\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.6091 - accuracy: 0.1172\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.5104 - accuracy: 0.1223\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.4209 - accuracy: 0.1268\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.3368 - accuracy: 0.1323\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.2572 - accuracy: 0.1366\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.1789 - accuracy: 0.1391\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.1067 - accuracy: 0.1438\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.0326 - accuracy: 0.1484\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.9644 - accuracy: 0.1526\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.9003 - accuracy: 0.1561\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.8413 - accuracy: 0.1590\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.7784 - accuracy: 0.1637\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.7190 - accuracy: 0.1659\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.6623 - accuracy: 0.1697\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.6101 - accuracy: 0.1708\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.5585 - accuracy: 0.1716\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.5085 - accuracy: 0.1755\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.4581 - accuracy: 0.1776\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.4110 - accuracy: 0.1814\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.3628 - accuracy: 0.1833\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.3174 - accuracy: 0.1853\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.2718 - accuracy: 0.1888\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.2271 - accuracy: 0.1918\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.1861 - accuracy: 0.1938\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.1433 - accuracy: 0.1970\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.1026 - accuracy: 0.2003\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.0646 - accuracy: 0.2034\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.0220 - accuracy: 0.2063\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.9844 - accuracy: 0.2092\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.9458 - accuracy: 0.2119\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.9100 - accuracy: 0.2146\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.8732 - accuracy: 0.2166\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.8410 - accuracy: 0.2200\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.8086 - accuracy: 0.2236\n",
      "Epoch 43/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.7673 - accuracy: 0.2266\n",
      "Epoch 44/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.7405 - accuracy: 0.2308\n",
      "Epoch 45/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.7092 - accuracy: 0.2336\n",
      "Epoch 46/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.6775 - accuracy: 0.2371\n",
      "Epoch 47/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.6451 - accuracy: 0.2409\n",
      "Epoch 48/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.6170 - accuracy: 0.2455\n",
      "Epoch 49/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.5874 - accuracy: 0.2476\n",
      "Epoch 50/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.5589 - accuracy: 0.2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 06:26:20.128854: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n",
      "2022-04-25 06:26:23.374421: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 7.6973 - accuracy: 0.0568\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.5993 - accuracy: 0.0748\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.3681 - accuracy: 0.0885\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.2036 - accuracy: 0.0920\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.0630 - accuracy: 0.0981\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.9411 - accuracy: 0.1061\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.8288 - accuracy: 0.1124\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.7178 - accuracy: 0.1199\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.6139 - accuracy: 0.1255\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.5132 - accuracy: 0.1331\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.4259 - accuracy: 0.1405\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.3373 - accuracy: 0.1465\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.2546 - accuracy: 0.1514\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.1779 - accuracy: 0.1568\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.1037 - accuracy: 0.1644\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.0329 - accuracy: 0.1678\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.9610 - accuracy: 0.1739\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.8938 - accuracy: 0.1794\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.8283 - accuracy: 0.1851\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.7628 - accuracy: 0.1904\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.7013 - accuracy: 0.1957\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.6406 - accuracy: 0.2006\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.5818 - accuracy: 0.2056\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.5240 - accuracy: 0.2102\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.4692 - accuracy: 0.2146\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.4118 - accuracy: 0.2198\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.3597 - accuracy: 0.2235\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.3062 - accuracy: 0.2293\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.2546 - accuracy: 0.2333\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.2008 - accuracy: 0.2378\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.1524 - accuracy: 0.2420\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.1031 - accuracy: 0.2455\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.0548 - accuracy: 0.2499\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.0043 - accuracy: 0.2546\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.9600 - accuracy: 0.2572\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.9126 - accuracy: 0.2616\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.8683 - accuracy: 0.2673\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.8281 - accuracy: 0.2698\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.7849 - accuracy: 0.2735\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.7400 - accuracy: 0.2787\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.6971 - accuracy: 0.2832\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.6549 - accuracy: 0.2859\n",
      "Epoch 43/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.6184 - accuracy: 0.2892\n",
      "Epoch 44/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.5766 - accuracy: 0.2952\n",
      "Epoch 45/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.5422 - accuracy: 0.2979\n",
      "Epoch 46/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.5023 - accuracy: 0.3029\n",
      "Epoch 47/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.4633 - accuracy: 0.3061\n",
      "Epoch 48/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.4305 - accuracy: 0.3105\n",
      "Epoch 49/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.3926 - accuracy: 0.3149\n",
      "Epoch 50/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.3597 - accuracy: 0.3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 06:30:30.681244: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 7.5849 - accuracy: 0.0503\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.8470 - accuracy: 0.0711\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.6282 - accuracy: 0.0798\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.4752 - accuracy: 0.0845\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.3478 - accuracy: 0.0889\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.2348 - accuracy: 0.0938\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.1286 - accuracy: 0.1010\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 6.0256 - accuracy: 0.1084\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.9323 - accuracy: 0.1152\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.8348 - accuracy: 0.1244\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.7443 - accuracy: 0.1309\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.6569 - accuracy: 0.1373\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.5618 - accuracy: 0.1430\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.4711 - accuracy: 0.1490\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.3830 - accuracy: 0.1553\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.3015 - accuracy: 0.1628\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.2233 - accuracy: 0.1693\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.1508 - accuracy: 0.1760\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.0790 - accuracy: 0.1818\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 5.0103 - accuracy: 0.1877\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.9466 - accuracy: 0.1924\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.8852 - accuracy: 0.2002\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.8274 - accuracy: 0.2055\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.7680 - accuracy: 0.2116\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.7156 - accuracy: 0.2182\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.6587 - accuracy: 0.2227\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.6062 - accuracy: 0.2277\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.5548 - accuracy: 0.2335\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.5085 - accuracy: 0.2376\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.4614 - accuracy: 0.2428\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.4159 - accuracy: 0.2473\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.4028 - accuracy: 0.2502\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.3389 - accuracy: 0.2540\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.2892 - accuracy: 0.2592\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.2506 - accuracy: 0.2639\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.2093 - accuracy: 0.2676\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.1699 - accuracy: 0.2722\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.1354 - accuracy: 0.2749\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.0963 - accuracy: 0.2793\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.0620 - accuracy: 0.2820\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 4.0272 - accuracy: 0.2848\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.9918 - accuracy: 0.2900\n",
      "Epoch 43/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.9625 - accuracy: 0.2920\n",
      "Epoch 44/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.9256 - accuracy: 0.2958\n",
      "Epoch 45/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.8870 - accuracy: 0.3005\n",
      "Epoch 46/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.8644 - accuracy: 0.3016\n",
      "Epoch 47/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.8340 - accuracy: 0.3052\n",
      "Epoch 48/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.7995 - accuracy: 0.3082\n",
      "Epoch 49/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.7726 - accuracy: 0.3106\n",
      "Epoch 50/50\n",
      "342/342 [==============================] - 5s 14ms/step - loss: 3.7430 - accuracy: 0.3141\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_490/894772583.py\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# save the model to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_DaVinci.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# save the tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizer_DaVinci.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "size_data = len(sequences2)//3\n",
    "\n",
    "for i in range(size_data,len(sequences2)+1, size_data):\n",
    "    sub_sequences2 = sequences2[i-size_data:i]\n",
    "    sequences_aux2 = array(sub_sequences2)\n",
    "    \n",
    "    X, y = sequences_aux2[:,:-1], sequences_aux2[:,-1]\n",
    "    y = to_categorical(y, num_classes=vocab_size2)\n",
    "\n",
    "    model2.fit(X, y, batch_size=128, epochs=50)\n",
    "    \n",
    "# save the model to file\n",
    "model2.save('model_DaVinci.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer2, open('tokenizer_DaVinci.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus 2 - Segundo Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 50, 50)            774550    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 50, 80)            41920     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 50, 65)            37960     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 50)                23200     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 15491)             790041    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,670,221\n",
      "Trainable params: 1,670,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "seq_length2 = 50 #X.shape[1]\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(vocab_size2, 50, input_length=seq_length2))\n",
    "model4.add(LSTM(80, return_sequences=True))\n",
    "model4.add(LSTM(65, return_sequences=True))\n",
    "model4.add(LSTM(50))\n",
    "model4.add(Dense(50, activation='relu'))\n",
    "model4.add(Dense(vocab_size2, activation='softmax'))\n",
    "print(model4.summary())\n",
    "# compile model\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 07:18:13.370810: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n",
      "2022-04-25 07:18:16.637312: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "342/342 [==============================] - 11s 17ms/step - loss: 7.2868 - accuracy: 0.0519\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 6.7698 - accuracy: 0.0541\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 6.7060 - accuracy: 0.0540\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 6.5476 - accuracy: 0.0642\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 6.3261 - accuracy: 0.0772\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 6.1445 - accuracy: 0.0801\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.0117 - accuracy: 0.0824\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 5.9026 - accuracy: 0.0851\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 5.8010 - accuracy: 0.0875\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.7057 - accuracy: 0.0922\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.6119 - accuracy: 0.0992\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 5.5199 - accuracy: 0.1092\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.4342 - accuracy: 0.1180\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.3504 - accuracy: 0.1259\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 5.2682 - accuracy: 0.1297\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.1899 - accuracy: 0.1353\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.1138 - accuracy: 0.1393\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.0359 - accuracy: 0.1443\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 4.9615 - accuracy: 0.1507\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.8879 - accuracy: 0.1561\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.8178 - accuracy: 0.1590\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 4.7491 - accuracy: 0.1661\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.6837 - accuracy: 0.1696\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.6229 - accuracy: 0.1745\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.7240 - accuracy: 0.1714\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 4.7398 - accuracy: 0.1668\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.5915 - accuracy: 0.1742\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.5182 - accuracy: 0.1781\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 4.4177 - accuracy: 0.1840\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.3461 - accuracy: 0.1908\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.2909 - accuracy: 0.1954\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.2322 - accuracy: 0.1999\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 4.1780 - accuracy: 0.2030\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.1284 - accuracy: 0.2063\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 4.0841 - accuracy: 0.2118\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.0358 - accuracy: 0.2150\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.0081 - accuracy: 0.2174\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.9668 - accuracy: 0.2231\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.9144 - accuracy: 0.2279\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8659 - accuracy: 0.2316\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8191 - accuracy: 0.2363\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.7794 - accuracy: 0.2400\n",
      "Epoch 43/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.9307 - accuracy: 0.2276\n",
      "Epoch 44/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8462 - accuracy: 0.2330\n",
      "Epoch 45/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.7591 - accuracy: 0.2407\n",
      "Epoch 46/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 3.7286 - accuracy: 0.2468\n",
      "Epoch 47/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6557 - accuracy: 0.2521\n",
      "Epoch 48/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.9655 - accuracy: 0.2248\n",
      "Epoch 49/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.0542 - accuracy: 0.2095\n",
      "Epoch 50/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.9411 - accuracy: 0.2221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 07:23:06.554553: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n",
      "2022-04-25 07:23:09.801661: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 7.1229 - accuracy: 0.0738\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.2429 - accuracy: 0.0878\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.0068 - accuracy: 0.0962\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.8553 - accuracy: 0.1047\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 6s 16ms/step - loss: 5.7233 - accuracy: 0.1105\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.6031 - accuracy: 0.1165\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.4906 - accuracy: 0.1232\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.3821 - accuracy: 0.1289\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.2737 - accuracy: 0.1353\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.1702 - accuracy: 0.1441\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.0677 - accuracy: 0.1519\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.9735 - accuracy: 0.1593\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.8761 - accuracy: 0.1653\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.7841 - accuracy: 0.1736\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.6883 - accuracy: 0.1808\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.5964 - accuracy: 0.1879\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.5073 - accuracy: 0.1969\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.4200 - accuracy: 0.2009\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.3367 - accuracy: 0.2078\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.2490 - accuracy: 0.2170\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.1720 - accuracy: 0.2205\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.0844 - accuracy: 0.2281\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.0044 - accuracy: 0.2354\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.9263 - accuracy: 0.2418\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8495 - accuracy: 0.2493\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.7680 - accuracy: 0.2558\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6920 - accuracy: 0.2631\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6151 - accuracy: 0.2704\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.5433 - accuracy: 0.2794\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.4695 - accuracy: 0.2863\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.4209 - accuracy: 0.2930\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.3312 - accuracy: 0.3018\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.2598 - accuracy: 0.3101\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.1881 - accuracy: 0.3196\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.1828 - accuracy: 0.3229\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.1230 - accuracy: 0.3335\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.4303 - accuracy: 0.3092\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.2916 - accuracy: 0.3231\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.1976 - accuracy: 0.3346\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.1049 - accuracy: 0.3456\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.9360 - accuracy: 0.3599\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.8308 - accuracy: 0.3827\n",
      "Epoch 43/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.7752 - accuracy: 0.3890\n",
      "Epoch 44/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.7368 - accuracy: 0.3990\n",
      "Epoch 45/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.7007 - accuracy: 0.4028\n",
      "Epoch 46/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.6400 - accuracy: 0.4118\n",
      "Epoch 47/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.5889 - accuracy: 0.4224\n",
      "Epoch 48/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.5478 - accuracy: 0.4291\n",
      "Epoch 49/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.5709 - accuracy: 0.4325\n",
      "Epoch 50/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 2.5397 - accuracy: 0.4376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 07:27:56.795111: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2710367324 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 7.5222 - accuracy: 0.0424\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.5311 - accuracy: 0.0762\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.3182 - accuracy: 0.0793\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.1694 - accuracy: 0.0838\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 6.0433 - accuracy: 0.0899\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.9314 - accuracy: 0.0977\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.9675 - accuracy: 0.1021\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.8792 - accuracy: 0.1068\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.7447 - accuracy: 0.1145\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.6095 - accuracy: 0.1202\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.4946 - accuracy: 0.1286\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.3899 - accuracy: 0.1325\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.3015 - accuracy: 0.1406\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.3004 - accuracy: 0.1414\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.2517 - accuracy: 0.1451\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.1741 - accuracy: 0.1506\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 5.0348 - accuracy: 0.1585\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.8954 - accuracy: 0.1696\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.7636 - accuracy: 0.1793\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.6509 - accuracy: 0.1897\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.7211 - accuracy: 0.1862\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.6807 - accuracy: 0.1912\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.6276 - accuracy: 0.1955\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.5970 - accuracy: 0.1991\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.4174 - accuracy: 0.2167\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.2791 - accuracy: 0.2276\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.1772 - accuracy: 0.2388\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.0911 - accuracy: 0.2444\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.0482 - accuracy: 0.2502\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.0983 - accuracy: 0.2471\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.9943 - accuracy: 0.2572\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.9042 - accuracy: 0.2690\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8280 - accuracy: 0.2777\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.7719 - accuracy: 0.2845\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8398 - accuracy: 0.2801\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8768 - accuracy: 0.2787\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8158 - accuracy: 0.2841\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6840 - accuracy: 0.2978\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6235 - accuracy: 0.3020\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6486 - accuracy: 0.3058\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6321 - accuracy: 0.3069\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8522 - accuracy: 0.2882\n",
      "Epoch 43/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.7195 - accuracy: 0.2042\n",
      "Epoch 44/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.4075 - accuracy: 0.2288\n",
      "Epoch 45/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 4.2664 - accuracy: 0.2405\n",
      "Epoch 46/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.8216 - accuracy: 0.2833\n",
      "Epoch 47/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.7796 - accuracy: 0.2904\n",
      "Epoch 48/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6518 - accuracy: 0.3065\n",
      "Epoch 49/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6544 - accuracy: 0.3100\n",
      "Epoch 50/50\n",
      "342/342 [==============================] - 6s 17ms/step - loss: 3.6950 - accuracy: 0.3060\n"
     ]
    }
   ],
   "source": [
    "size_data = len(sequences2)//3\n",
    "\n",
    "for i in range(size_data,len(sequences2)+1, size_data):\n",
    "    sub_sequences2 = sequences2[i-size_data:i]\n",
    "    sequences_aux2 = array(sub_sequences2)\n",
    "    \n",
    "    X, y = sequences_aux2[:,:-1], sequences_aux2[:,-1]\n",
    "    y = to_categorical(y, num_classes=vocab_size2)\n",
    "\n",
    "    model4.fit(X, y, batch_size=128, epochs=50)\n",
    "    \n",
    "# save the model to file\n",
    "model4.save('model2_DaVinci.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer2, open('tokenizer2_DaVinci.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        predict_yhat = model.predict(encoded, verbose=0)\n",
    "        yhat = argmax(predict_yhat, axis=1)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus 1 - Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "in_filename = 'HarryPoter_CleanText.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model_HarryPotter.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer_HarryPotter.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el muchacho sabía que no podían hacerle daño, miró hacia atrás, preguntándose qué o a quién esperaba aquel hombre de repente, un cegador e irregular chorro de luz blanca surcó el aire. harry pensó que era un rayo, pero snape se había arrodillado y la varita se le había caído de\n",
      "\n",
      "la sala de piedra de los amigos se había llegado a la sala de piedra de los amigos se había llegado a la sala de piedra de los amigos se había llegado a la sala de piedra de los amigos se había llegado a la sala de piedra de los\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Corpus 1 - Modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "in_filename = 'HarryPoter_CleanText.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines2 = doc.split('\\n')\n",
    "seq_length2 = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model2 = load_model('model2_HarryPotter.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer2 = load(open('tokenizer2_HarryPotter.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se dio cuenta de que harry, ron, hermione y ginny lo miraban; los saludó con una seca cabezada y se dio la vuelta. así que ése es el pequeño scorpius murmuró ron. asegúrate de superarlo en todos los exámenes, rosie. suerte que has heredado la inteligencia de tu madre. haz el\n",
      "\n",
      "dolor reconoció de que no había hecho que los menesteres la gente se ha quedado porque los sido las patas mientras la amor un poco detenerlo dijo harry harry se había utilizado de que los dos se había muerto la aparición harry y los mortífagos mientras se había muerto a\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text2 = lines2[randint(0,len(lines2))]\n",
    "print(seed_text2 + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model2, tokenizer2, seq_length2, seed_text2, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus 2 - Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "in_filename = 'CodigoDaVinci_CleanText.txt'\n",
    "doc3 = load_doc(in_filename)\n",
    "lines3 = doc3.split('\\n')\n",
    "seq_length3 = len(lines3[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model3 = load_model('model_DaVinci.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer3 = load(open('tokenizer_DaVinci.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "falso, o los siguientes serán sophie y usted. fue un accidente de coche intervino sophie, que notaba que el dolor de su infancia volvía a apoderarse de ella. ¡un accidente! cuentos infantiles para proteger su inocencia replicó teabing. piense que sólo dos miembros de la familia quedaron con vida, el gran\n",
      "\n",
      "maestre como que se acercó más con la puerta que se lo la tumba de la puerta que se lo la tumba de la puerta que se lo londres en la mano y se le lo londres en la tumba de la puerta que se lo la tumba de la\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text3 = lines3[randint(0,len(lines3))]\n",
    "print(seed_text3 + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated3 = generate_seq(model3, tokenizer3, seq_length3, seed_text3, 50)\n",
    "print(generated3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus 2 - Modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "in_filename = 'CodigoDaVinci_CleanText.txt'\n",
    "doc4 = load_doc(in_filename)\n",
    "lines4 = doc4.split('\\n')\n",
    "seq_length4 = len(lines4[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model4 = load_model('model2_DaVinci.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer4 = load(open('tokenizer2_DaVinci.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la oscuridad. el brillo tenue y rojizo de las luces de emergencia apuntaba hacia arriba, iluminando con un resplandor artificial la colección de leonardos, tizianos y caravaggios suspendidos del techo con cables. naturalezas muertas, escenas religiosas y paisajes se alternaban con retratos de nobles y políticos. aunque la gran galería albergaba\n",
      "\n",
      "el grial del grial no recordó que lo echarle un gesto en un reverso y que se había olvidado de que los ojos que se había dicho que había sido un momento y se había olvidado de cinco hurtos y los ojos y se ver que no había salido y\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text4 = lines4[randint(0,len(lines4))]\n",
    "print(seed_text4 + '\\n')\n",
    "\n",
    "# generate new text\n",
    "generated4 = generate_seq(model4, tokenizer4, seq_length4, seed_text4, 50)\n",
    "print(generated4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
